\section{Empirical Comparison}
\label{sec:empirical_comparison}

We now provide a detailed empirical comparison for the advantage and
disadvantage of the data transfer methods presented in
Section~\ref{sec:data_transfer_methods}.
Our experimental setup is composed of an Intel Core i7 2600 processor
and an NVIDIA GeForce GTX~480 graphics card.
We use the Linux kernel v2.6.42 and Gdev~\cite{Kato_ATC12} as the
underlying OS and GPGPU runtime/driver software respectively.
This set of open-source platforms allows our implementations of the
investigated data transfer methods.

The test programs are written in CUDA~\cite{NVIDIA_CUDA} and are compiled
using the NVIDIA CUDA Compiler (NVCC) v4.2~\cite{NVIDIA_NVCC}.
Note that Gdev is compatible with this binary compiler toolkit.
We exclude compute kernels and focus on data transfer functions in this
empirical comparison.
While the test programs uniformly use the same CUDA API functions, we
provide different internal implementations according to the target data
transfer methods.

Data streams between the host and the device memory are provided by
real-time tasks in Linux.
These real-time tasks are prioritized over other background tasks
running in Linux.
The real-time capability relies on the default performance of the
real-time scheduling class supported by the Linux kernel.
We believe that this setup is sufficient for our experiments given that
we execute at most one real-time task in the system while multiple data
streams may be produced by this task.
Scheduling performance issues are not within the scope of this paper.

Henchforth we use the following labels to denote the investigated data
transfer methods respectively:
\begin{itemize}
 \item \textbf{DMA} denotes the standard DMA method presented in
       Section~\ref{sec:dma}.
 \item \textbf{IORW} denotes the memory-mapped read and write method
       presented in Section~\ref{sec:iorw}.
 \item \textbf{MEMWND} denotes the memory-window read and write method
       presented in Section~\ref{sec:memwnd}.
 \item \textbf{HUB} denotes the microcontroller-based data transfer
       method presented in Section~\ref{sec:micro}, particularly using a
       \textit{hub} microcontroller designed to broadcast among the
       actual microcontrollers of graphics processing clusters (GPCs),
       \textit{i.e.}, CUDA core clusters.
 \item \textbf{GPC} denotes the microcontroller-based data transfer
       method presented in Section~\ref{sec:micro}, particularly using a
       single GPC microcontroller.
 \item \textbf{GPC4} denotes the microcontroller-based data transfer
       method presented in Section~\ref{sec:micro}, particularly using
       four different GPC microcontrollers in parallel.
       Note that the NVIDIA Fermi architecture~\cite{NVIDIA_Fermi}
       provides four GPCs and their microcontrollers can perform
       invididually.
       Therefore we can split the data transfer into four pieces and
       make the four microcontrollers work in parallel.
\end{itemize}

\subsection{Basic Performance}

\begin{figure}[!t]
 \begin{center}
  \subfigure[Host to Device]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_normal_HtoD.pdf}}\\
  \subfigure[Device to Host]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_normal_DtoH.pdf}}
  \caption{Average performance of single streams.}
  \label{fig:average_transfer_real-time}
 \end{center}
 \vspace{0.5em}
 \begin{center}
  \subfigure[Host to Device]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_normal_HtoD_worst.pdf}}\\
  \subfigure[Device to Host]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_normal_DtoH_worst.pdf}}
  \caption{Worst-case performance of single streams.}
  \label{fig:worst_case_transfer_real-time}
 \end{center}
\end{figure}

Figure~\ref{fig:average_transfer_real-time} shows the average
performance of each data transfer method when a single data stream
performs alone.
Even with this most straightforward setup, there are several interesting
observations.
Performance characteristics of the host-to-device and the device-to-host
communications are not identical at all.
In particular, the performance of standard DMA exhibits a 10x difference
between the two directions of communications.
This is attributed to hardware capabilities that are not well documented
to the public.
We also find that performances of the methods are diverse but a
combination of \textsf{DMA} and \textsf{IORW} can derive the best
performance in this setup.
The other methods are almost always inferior to either of \textsf{DMA}
or \textsf{IORW}.
The most signficant finding is that \textsf{IORW} becomes very slow for
the device-to-host direction.
We consider that this is due to a design specification of the GPU.
It is designed so that the GPU can read data fast from the host computer
but conpromise write access performance.
Another interesting observation is that using multiple GPC
microcontrollers to parallelize the data trasfer is less effective than
a single GPC or HUB controller when the data size is small.

Figure~\ref{fig:worst_case_transfer_real-time} shows the worst-case
performance in the same setup as the above experiment.
It is important to note that we acquire almost the same results as those
shown in Figure~\ref{fig:average_transfer_real-time}, though there is
some degradation in the performance of \textsf{DMA} for the
host-to-device direction.
These comparisons lead to some conclusion that we may optimize the data
transfer performance by switching between \textsf{DMA} and \textsf{IORW}
at an appropriate boundary.

\subsection{Interferred Performance}

\begin{figure}[!t]
 \begin{center}
  \subfigure[Host to Device]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_cpuload_HtoD.pdf}}\\
  \subfigure[Device to Host]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_cpuload_DtoH.pdf}}
  \caption{Average performance of single streams under high CPU load.}
  \label{fig:average_transfer_cpuload_real-time}
 \end{center}
 \vspace{0.5em}
 \begin{center}
  \subfigure[Host to Device]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_cpuload_HtoD_worst.pdf}}\\
  \subfigure[Device to Host]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_cpuload_DtoH_worst.pdf}}
  \caption{Worst-case performance of single streams under high CPU load.}
  \label{fig:worst_case_data_transfer_cpuload_real-time}
  \end{center}
\end{figure}

\begin{figure}[!t]
 \begin{center}
  \subfigure[Host to Device]{\includegraphics[width=0.34\textwidth]{figure/Graph/not_realtask/Memcpy_cpuload_HtoD.pdf}}\\
  \subfigure[Device to Host]{\includegraphics[width=0.34\textwidth]{figure/Graph/not_realtask/Memcpy_cpuload_DtoH.pdf}}
  \caption{Average performance of \textsf{NRT} single streams under high
  CPU load.}
  \label{fig:average_data_transfer_cpuload}
 \end{center}
 \vspace{0.5em}
 \begin{center}
  \subfigure[Host to Device]{\includegraphics[width=0.34\textwidth]{figure/Graph/not_realtask/Memcpy_cpuload_HtoD_worst.pdf}}\\
  \subfigure[Device to Host]{\includegraphics[width=0.34\textwidth]{figure/Graph/not_realtask/Memcpy_cpuload_DtoH_worst.pdf}}
  \caption{Worst-case performance of \textsf{NRT} single streams under
  high CPU load.}
  \label{fig:worst_case_data_transfer_cpuload}
 \end{center}
\end{figure}

Figure~\ref{fig:average_transfer_cpuload_real-time} shows the average
performance of each data transfer method when the CPU encounters
extremely high workload.


\begin{figure}[!t]
 \centering
 \subfigure[Host to Device]{\includegraphics[width=0.4\textwidth]{figure/Graph/realtask/Memcpy_rtask_memswap_HtoD.pdf}}\\
 \subfigure[Device to Host]{\includegraphics[width=0.4\textwidth]{figure/Graph/realtask/Memcpy_rtask_memswap_DtoH.pdf}}
 \caption{Average data transfer times of real-time single streams
 under high memory access load.}
 \label{fig:average_data_transfer_memswap_real-time}
\end{figure}

\begin{figure}[!t]
 \centering
 \subfigure[Host to Device]{\includegraphics[width=0.4\textwidth]{figure/Graph/realtask/Memcpy_rtask_memswap_HtoD_worst.pdf}}\\
 \subfigure[Device to Host]{\includegraphics[width=0.4\textwidth]{figure/Graph/realtask/Memcpy_rtask_memswap_DtoH_worst.pdf}}
 \caption{Worst-case data transfer times of real-time single streams
 under high memory access load.}
 \label{fig:worst_case_data_transfer_memswap_real-time}
\end{figure}

\begin{figure}[!t]
 \centering
 \subfigure[Host to Device]{\includegraphics[width=0.4\textwidth]{figure/Graph/realtask/Memcpy_rtask_hackbench_HtoD.pdf}}\\
 \subfigure[Device to Host]{\includegraphics[width=0.4\textwidth]{figure/Graph/realtask/Memcpy_rtask_hackbench_DtoH.pdf}}
 \caption{Average data transfer times of real-time single streams in the
 presence of \textsf{hackbench}.}
 \label{fig:average_data_transfer_hackbench_real-time}
\end{figure}

\begin{figure}[!t]
 \centering
 \subfigure[Host to Device]{\includegraphics[width=0.4\textwidth]{figure/Graph/realtask/Memcpy_rtask_hackbench_HtoD_worst.pdf}}\\
 \subfigure[Device to Host]{\includegraphics[width=0.4\textwidth]{figure/Graph/realtask/Memcpy_rtask_hackbench_DtoH_worst.pdf}}
 \caption{Worst-case data transfer times of real-time single streams in
 the presence of \textsf{hackbench}.}
 \label{fig:worst_case_data_transfer_hackbench_real-time}
\end{figure}

\begin{figure}[!t]
 \centering
 \subfigure[Host to Device]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_hybrid_HtoD.pdf}}\\
 \subfigure[Device to Host]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_hybrid_DtoH.pdf}}\\
 \caption{Average data transfer times of concurrent two streams.}
 \label{fig:average_data_transfer_concurrent}
\end{figure}

\begin{comment}
\begin{figure}[!t]
 \centering
 \subfigure[Host to Device]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_normal_HtoD.pdf}}\\
 \subfigure[Device to Host]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_normal_DtoH.pdf}}
 \caption{Average data transfer times of single streams.}
 \label{fig:average_data_transfer}
\end{figure}

\begin{figure}[!t]
 \centering
 \subfigure[Host to Device]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_normal_HtoD_worst.pdf}}\\
 \subfigure[Device to Host]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_normal_DtoH_worst.pdf}}
 \caption{Worst-case data transfer times of single streams.}
 \label{fig:worst_case_data_transfer}
\end{figure}

\begin{figure}[!t]
 \centering
 \subfigure[Host to Device]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_memswap_HtoD.pdf}}\\
 \subfigure[Device to Host]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_memswap_DtoH.pdf}}
 \caption{Average data transfer times of single streams under high
 memory access load.} 
 \label{fig:average_data_transfer_memswap}
\end{figure}

\begin{figure}[!t]
 \centering
 \subfigure[Host to Device]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_memswap_HtoD_worst.pdf}}\\
 \subfigure[Device to Host]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_memswap_DtoH_worst.pdf}}
 \caption{Worst-case data transfer times of single streams under high
 memory access load.}
 \label{fig:worst_case_data_transfer_memswap}
\end{figure}

\begin{figure}[!t]
 \centering
 \subfigure[Host to Device]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_hackbench_HtoD.pdf}}\\
 \subfigure[Device to Host]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_hackbench_DtoH.pdf}}
 \caption{Average data transfer times of single streams in the presence
 of \textsf{hackbench}.}
 \label{fig:average_data_transfer_hackbench}
\end{figure}

\begin{figure}[!t]
 \centering
 \subfigure[Host to Device]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_hackbench_HtoD_worst.pdf}}\\
 \subfigure[Device to Host]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_hackbench_DtoH_worst.pdf}}
 \caption{Worst-case data transfer times of single streams in the
 presence of \textsf{hackbench}.}
 \label{fig:worst_case_data_transfer_hackbench}
\end{figure}
\end{comment}