\section{Empirical Comparison}
\label{sec:empirical_comparison}

We now provide a detailed empirical comparison for the advantage and
disadvantage of the data transfer methods presented in
Section~\ref{sec:data_transfer_methods}.
Our experimental setup is composed of an Intel Core i7 2600 processor
and an NVIDIA GeForce GTX~480 graphics card.
We use the Linux kernel v2.6.42 and Gdev~\cite{Kato_ATC12} as the
underlying OS and GPGPU runtime/driver software respectively.
This set of open-source platforms allows our implementations of the
investigated data transfer methods.

The test programs are written in CUDA~\cite{NVIDIA_CUDA} and are compiled
using the NVIDIA CUDA Compiler (NVCC) v4.2~\cite{NVIDIA_NVCC}.
Note that Gdev is compatible with this binary compiler toolkit.
We exclude compute kernels and focus on data transfer functions in this
empirical comparison.
While the test programs uniformly use the same CUDA API functions, we
provide different internal implementations according to the target data
transfer methods.

Data streams between the host and the device memory are provided by a
single GPU context.
Performance interference among multiple GPU contexts is outside the
scope of this paper.
We evaluate the data transfer performances of both real-time and normal
tasks.
For the scheduling policies of the Linux kernel, we use
\texttt{SCHED\_FIFO} for real-time tasks while \texttt{SCHED\_OTHER} for
normal tasks, where the real-time tasks are always prioritized over the
normal tasks.
The real-time capability relies on the default performance of the
real-time scheduling class supported by the Linux kernel.
We believe that this setup is sufficient for our experiments given that
we execute at most one real-time task in the system while multiple data
streams may be produced by this task.
Overall the scheduling performance issues are outside the scope of this
paper.

Henceforth we use the following labels to denote the investigated data
transfer methods respectively:
\begin{itemize}
 \item \textbf{DMA} denotes the standard DMA method presented in
       Section~\ref{sec:dma}.
 \item \textbf{IORW} denotes the memory-mapped read and write method
       presented in Section~\ref{sec:iorw}.
 \item \textbf{MEMWND} denotes the memory-window read and write method
       presented in Section~\ref{sec:memwnd}.
 \item \textbf{HUB} denotes the microcontroller-based data transfer
       method presented in Section~\ref{sec:micro}, particularly using a
       \textit{hub} microcontroller designed to broadcast among the
       actual microcontrollers of graphics processing clusters (GPCs),
       \textit{i.e.}, CUDA core clusters.
 \item \textbf{GPC} denotes the microcontroller-based data transfer
       method presented in Section~\ref{sec:micro}, particularly using a
       single GPC microcontroller.
 \item \textbf{GPC4} denotes the microcontroller-based data transfer
       method presented in Section~\ref{sec:micro}, particularly using
       four different GPC microcontrollers in parallel.
       Note that the NVIDIA Fermi architecture~\cite{NVIDIA_Fermi}
       provides four GPCs and their microcontrollers can perform
       individually.
       Therefore we can split the data transfer into four pieces and
       make the four microcontrollers work in parallel.
\end{itemize}

\subsection{Basic Performance}
\label{sec:basic_performance}

\begin{figure}[!t]
 \begin{center}
  \subfigure[Host to Device]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_normal_HtoD.pdf}}\\
  \subfigure[Device to Host]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_normal_DtoH.pdf}}
  \caption{Average performance of each data transfer method with a
  real-time task.}
  \label{fig:average_realtime}
 \end{center}
 \vspace{0.5em}
 \begin{center}
  \subfigure[Host to Device]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_normal_HtoD_worst.pdf}}\\
  \subfigure[Device to Host]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_normal_DtoH_worst.pdf}}
  \caption{Worst-case performance of each data transfer method with a
  real-time task.}
  \label{fig:worst_realtime}
 \end{center}
\end{figure}

Figure~\ref{fig:average_realtime} shows the average performance of each
data transfer method when a real-time task runs alone.
Even with this most straightforward setup, there are several interesting
observations.
Performance characteristics of the host-to-device and the device-to-host
communications are not identical at all.
In particular the performance of standard DMA exhibits a 10x difference
between the two directions of communications.
We believe that this is due to hardware capabilities that are not
documented to the public.
We also find that the performances of different methods are diverse but
either \textsf{DMA} or \textsf{IORW} can derive the best performance for
any data size.
In case of the host-to-device direction, for small data from $16$B to
$8$MB, \textsf{IORW} is preferred, while \textsf{DMA} outperforms
\textsf{IORW} larger data than $8$MB.
The other methods are almost always inferior to either of \textsf{DMA}
or \textsf{IORW}.
The most significant finding is that \textsf{IORW} becomes very slow for
the device-to-host direction.
It is faster than \textsf{DMA} only until $512$B.
This is due to a design specification of the GPU.
It is designed so that the GPU can read data fast from the host computer
but compromise write access performance.
Another interesting observation is that using multiple GPC
microcontrollers to parallelize the data transfer is less effective than
a single GPC or HUB controller when the data size is small.

Figure~\ref{fig:worst_realtime} shows the worst-case performance in
the same setup as the above experiment.
It is important to note that we acquire almost the same results as those
shown in Figure~\ref{fig:average_realtime}, though there is
some degradation in the performance of \textsf{DMA} for the
host-to-device direction.
These comparisons lead to some conclusion that we may optimize the data
transfer performance by switching between \textsf{DMA} and \textsf{IORW}
at an appropriate boundary.

We omit the results of normal tasks in this setup, because they are
almost equal to those of real-time tasks shown above.
However, real-time and normal tasks behave in a very different manner in
the presence of competing workload.
This will be discussed in the next subsection.

\begin{comment}
\begin{figure}[!t]
 \begin{center}
  \subfigure[Host to Device]{\includegraphics[width=0.34\textwidth]{figure/Graph/not_realtask/Memcpy_normal_HtoD.pdf}}\\
  \subfigure[Device to Host]{\includegraphics[width=0.34\textwidth]{figure/Graph/not_realtask/Memcpy_normal_DtoH.pdf}}
  \caption{Average performance of each data transfer method with a
  normal task.}
  \label{fig:average_normal}
 \end{center}
 \vspace{0.5em}
 \begin{center}
  \subfigure[Host to Device]{\includegraphics[width=0.34\textwidth]{figure/Graph/not_realtask/Memcpy_normal_HtoD_worst.pdf}}\\
  \subfigure[Device to Host]{\includegraphics[width=0.34\textwidth]{figure/Graph/not_realtask/Memcpy_normal_DtoH_worst.pdf}}
  \caption{Worst-case performance of each data transfer methods with a
  normal task.}
  \label{fig:worst_normal}
 \end{center}
\end{figure}

Figure~\ref{fig:average_normal}~and~\ref{fig:worst_normal} show the
average and the worst-case performance of a normal in the same setup as
the above experiments.
\end{comment}

\subsection{Interfered Performance}
\label{sec:interfered_performance}

\begin{figure}[!t]
 \begin{center}
  \subfigure[Host to Device]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_cpuload_HtoD.pdf}}\\
  \subfigure[Device to Host]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_cpuload_DtoH.pdf}}
  \caption{Average performance of each data transfer method with a
  real-time task under high CPU load.}
  \label{fig:average_realtime_cpuload}
 \end{center}
 \vspace{0.5em}
 \begin{center}
  \subfigure[Host to Device]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_cpuload_HtoD_worst.pdf}}\\
  \subfigure[Device to Host]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_cpuload_DtoH_worst.pdf}}
  \caption{Worst-case performance of each data transfer methods with a
  real-time task under high CPU load.}
  \label{fig:worst_realtime_cpuload}
  \end{center}
\end{figure}

Figure~\ref{fig:average_realtime_cpuload} shows the average performance
of each data transfer method when a real-time task encounters extremely
high workload on the CPU.
All the methods are successfully protected from performance interference
due to the real-time scheduler.
Note that \textsf{DMA} shows a better performance than the previous
experiments despite the presence of competing workload.
This is due to the Linux real-time scheduler feature.
\textsf{DMA} is performed by GPU commands, which may impose a
suspension on the caller task.
In the Linux kernel, a real-time task is awakened in a more responsive
manner when switched from a normal task than from an idle task.
Therefore when the CPU is fully loaded by normal tasks, a real-time task
is more responsive.
The same is true for the worst-case performance as shown in
Figure~\ref{fig:worst_realtime_cpuload}.
We learn from these experiments that CPU priorities can protect the
performance of data transfer for the GPU.
Note that Gdev uses a polling approach to wait for completions of data
transfers.
An interrupt approach is also worth being investigated.

\begin{figure}[!t]
 \begin{center}
  \subfigure[Host to Device]{\includegraphics[width=0.34\textwidth]{figure/Graph/not_realtask/Memcpy_cpuload_HtoD.pdf}}\\
  \subfigure[Device to Host]{\includegraphics[width=0.34\textwidth]{figure/Graph/not_realtask/Memcpy_cpuload_DtoH.pdf}}
  \caption{Average performance of each data transfer method with a
  normal data stream under high CPU load.}
  \label{fig:average_normal_cpuload}
 \end{center}
 \vspace{0.5em}
 \begin{center}
  \subfigure[Host to Device]{\includegraphics[width=0.34\textwidth]{figure/Graph/not_realtask/Memcpy_cpuload_HtoD_worst.pdf}}\\
  \subfigure[Device to Host]{\includegraphics[width=0.34\textwidth]{figure/Graph/not_realtask/Memcpy_cpuload_DtoH_worst.pdf}}
  \caption{Worst-case performance of each data transfer method with a
  normal data stream under high CPU load.}
  \label{fig:worst_normal_cpuload}
 \end{center}
\end{figure}

For reference,
Figure~\ref{fig:average_normal_cpuload}~and~\ref{fig:worst_normal_cpuload}
show the average and the worst-case performance achieved by a normal
task when the CPU encounters extremely high workload same as the
preceding experiments.
Apparently the data transfer times increase by orders of magnitude as
compared to those achieved by a real-time task.
\textsf{DMA} shows a bad performance for small data while it can sustain
that performance for large data.
This is attributed to the fact that once a DMA command is fired, the
data transfer does not have to compete with CPU workload.
The other methods are more or less controlled by the CPU, and thus are
more affected by CPU workload.
This finding provides a design choice for the system implementation.

\begin{figure}[!t]
 \begin{center}
  \subfigure[Host to Device]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_memswap_HtoD.pdf}}\\
  \subfigure[Device to Host]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_memswap_DtoH.pdf}}
  \caption{Average performance of each data transfer method with a
  real-time task under high memory pressure.}
  \label{fig:average_realtime_memswap}
 \end{center}
 \vspace{0.5em}
 \begin{center}
  \subfigure[Host to Device]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_memswap_HtoD_worst.pdf}}\\
  \subfigure[Device to Host]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_memswap_DtoH_worst.pdf}}
  \caption{Worst-case performance of each data transfer method with a
  real-time task under high memory pressure.}
  \label{fig:worst_realtime_memswap}
 \end{center}
\end{figure}

From now on, we restrict our attention to a real-time task.
We next evaluate the performance of each data transfer method under high
memory pressure, creating another task that eats up host memory space.
In some sense, this is not a meaningful experiment because we use the
pinned host memory space to allocate buffers while the memory pressure
is supposed to compel the paged host memory space.
Having said that, the memory pressure could still impose indirect
interference on real-time tasks~\cite{Kato_RTSJ11, Yang_OSDI08}.
It is worth conducting this experiment.
As demonstrated in
Figure~\ref{fig:average_realtime_memswap}~and~\ref{fig:worst_realtime_memswap},
the impact of memory pressure on the data transfer performance is
negligible.
This means that all the data transfer methods investigated in this paper
require not much paged host memory space.
Otherwise they must be interfered by memory workload.

\begin{figure}[!t]
 \begin{center}
  \subfigure[Host to Device]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_hackbench_HtoD.pdf}}\\
  \subfigure[Device to Host]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_hackbench_DtoH.pdf}}
  \caption{Average performance of each data transfer method with a
  real-time task in the presence of \textsf{hackbench}.}
  \label{fig:average_realtime_hackbench}
 \end{center}
 \vspace{0.5em}
 \begin{center}
  \subfigure[Host to Device]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_hackbench_HtoD_worst.pdf}}\\
  \subfigure[Device to Host]{\includegraphics[width=0.34\textwidth]{figure/Graph/realtask/Memcpy_rtask_hackbench_DtoH_worst.pdf}}
  \caption{Worst-case performance of each data transfer method with a
  real-time task in the presence of \textsf{hackbench}.}
  \label{fig:worst_realtime_hackbench}
 \end{center}
\end{figure}

Figure~\ref{fig:average_realtime_hackbench}~and~\ref{fig:worst_realtime_hackbench}
show the average and the worst-case performance of each data transfer
method respectively, when a misbehaving \textsf{hackbench} process coexists.
\textsf{hackbench} is a tool that generates many processes executing I/O
system calls with pipes.
The results are all similar to the previous ones.
Since the Linux real-time scheduler is now well enhanced to protect a
real-time task from such misbehaving workload, these results are obvious
and trivial in some sense, but we can lead to a conclusion from a series
of the above experiments that \textit{the data transfer methods for the
GPU can be protected by the traditional real-time scheduler capability}.
This is a useful finding to facilitate an integration of real-time
systems and GPU computing.

\subsection{Concurrent Performance}

\begin{figure}[!t]
 \begin{center}
  \subfigure[Host to Device]{\includegraphics[width=0.34\textwidth]{figure/Graph/not_realtask/Memcpy_hybrid_HtoD.pdf}}\\
  \subfigure[Device to Host]{\includegraphics[width=0.34\textwidth]{figure/Graph/not_realtask/Memcpy_hybrid_DtoH.pdf}}\\
  \caption{Average performance of combinations of the data transfer
  methods for concurrent real-time tasks.}
  \label{fig:average_realtime_concurrent}
 \end{center}
 \vspace{0.5em}
 \begin{center}
  \subfigure[Host to Device]{\includegraphics[width=0.34\textwidth]{figure/Graph/not_realtask/Memcpy_hybrid_HtoD_worst.pdf}}\\
  \subfigure[Device to Host]{\includegraphics[width=0.34\textwidth]{figure/Graph/not_realtask/Memcpy_hybrid_DtoH_worst.pdf}}\\
  \caption{Worst-case performance of combinations of the data transfer
  methods for concurrent real-time tasks.}
  \label{fig:worst_realtime_concurrent}
 \end{center}
\end{figure}

So far we have studied the capabilities of the investigated data
transfer methods and their causal relation to a real-time task.
We now evaluate the performance of concurrent \textit{two} data streams
using different combinations of the data transfer methods as shown in
Figure~\ref{fig:average_realtime_concurrent}~and~\ref{fig:worst_realtime_concurrent}.
This is a very interesting result.
For the host-to-device direction, the best performance is obtained when
both the two tasks use \textsf{IORW}.
In this case, the two data streams are not overlapped but are processed
in sequential due to the use of the same \textsf{IORW} path.
Nonetheless it outperforms the other combinations because the
performance of \textsf{IORW} is way higher than the other methods as we
have observed in a series of the previous experiments.
However, the device-to-host direction shows a different performance.
Since \textsf{IORW} becomes slow when the CPU reads the device memory as
mentioned in Section~\ref{sec:basic_performance},
\textsf{IORW}/\textsf{IORW} is not the best performer any longer.
Instead using the microcontroller(s) provides the best performance until
$2$MB.
This is attributed to the fact that the microcontroller-based data
transfer method can be overlapped with any other data transfer methods.
From $16$B to $16$KB, a combination of the microcontroller and
\textsf{IORW} is the fastest, while that of the microcontroller and
\textsf{DMA} is the fastest from $16$KB to $2$MB.
Note that from $16$B to $16$KB the performance is aligned with the slow
\textsf{IORW} curve.
Therefore the choice of \textsf{HUB}, \textsf{GPC}, and \textsf{GPC4}
does not really matter to the performance.
However, from $16$KB to $2$MB the performance is improved by using four
microcontrollers in parallel (\textit{i.e.}, \textsf{GPC4}), since
\textsf{DMA} is faster than the microcontroller and
thereby the performance is aligned with the microcontroller curve.
We learn from this experiment that the microcontroller is useful to
overlap concurrent data streams with \textsf{DMA} or \textsf{IORW}, and
using multiple microcontrollers in parallel can further improve the
performance of concurrent data transfers.

\begin{comment}
\begin{figure}[!t]
 \centering
 \subfigure[Host to Device]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_memswap_HtoD.pdf}}\\
 \subfigure[Device to Host]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_memswap_DtoH.pdf}}
 \caption{Average data transfer times of single streams under high
 memory access load.} 
 \label{fig:average_data_transfer_memswap}
\end{figure}

\begin{figure}[!t]
 \centering
 \subfigure[Host to Device]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_memswap_HtoD_worst.pdf}}\\
 \subfigure[Device to Host]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_memswap_DtoH_worst.pdf}}
 \caption{Worst-case data transfer times of single streams under high
 memory access load.}
 \label{fig:worst_case_data_transfer_memswap}
\end{figure}

\begin{figure}[!t]
 \centering
 \subfigure[Host to Device]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_hackbench_HtoD.pdf}}\\
 \subfigure[Device to Host]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_hackbench_DtoH.pdf}}
 \caption{Average data transfer times of single streams in the presence
 of \textsf{hackbench}.}
 \label{fig:average_data_transfer_hackbench}
\end{figure}

\begin{figure}[!t]
 \centering
 \subfigure[Host to Device]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_hackbench_HtoD_worst.pdf}}\\
 \subfigure[Device to Host]{\includegraphics[width=0.4\textwidth]{figure/Graph/not_realtask/Memcpy_hackbench_DtoH_worst.pdf}}
 \caption{Worst-case data transfer times of single streams in the
 presence of \textsf{hackbench}.}
 \label{fig:worst_case_data_transfer_hackbench}
\end{figure}
\end{comment}