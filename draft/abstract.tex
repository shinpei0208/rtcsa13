\begin{abstract}
 Graphics processing units (GPUs) embrace many-core compute devices
 where massively parallelized compute kernels are offloaded from CPUs to
 GPUs.
 This heterogeneous nature of GPU computing raises non-trivial data
 transfer issues especially for low-latency real-time systems.
 A detailed mechanism of data transfers for GPU computing needs to be
 studied.
 In this paper, we investigate possible data transfer methods for the
 GPU in the state of the art.
 We also provide open-source implementations of these methods to
 compare their advantage and disadvantage in performance using
 real-world systems.
 Our experimental results show that the hardware-based direct memory
 access (DMA) and the I/O read and write methods are the most effective,
 while on-chip microcontrollers inside the GPU are useful to reduce the
 data transfer times of concurrent multiple data streams.
 Our findings also include that CPU priorities can protect the
 performance of GPU data transfers in the context of real-time systems.
\end{abstract}
