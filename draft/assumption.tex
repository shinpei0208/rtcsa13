\section{Assumption and Terminology}
\label{sec:assumption}

We assume the Compute Unified Device Architecture (CUDA) for GPU
programming~\cite{NVIDIA_CUDA}.
A unit of code that is individually launched on the GPU is called
a \textit{kernel}.
The kernel is composed of multiple \textit{threads} that execute the
code in parallel.
A unit of threads that are co-scheduled by hardware is called a
\textit{block}, while a collection of blocks for the corresponding
kernel is called a \textit{grid}.  
The maximum number of threads that can be contained by an individual
block is defined by the GPU architecture.

CUDA uses a set of an application programming interface (API) functions
to control the GPU.
A CUDA program often follows (i) allocate space
to the device memory, (ii) copy input data to the allocated device memory
space, (iii) launch the program on the GPU, (iv) copy output data back
to the host memory, and (v) free the allocated device memory space. 
The scope of this paper is related to (ii) and (iv).
In particular, we use the \texttt{cuMemCopyHtoD()} and the
\texttt{cuMemCopyDtoH()} functions of the CUDA Driver API, which
correspond to (ii) and (iv) respectively.
Since an open-source implementation of these functions is available with
Gdev~\cite{Kato_ATC12}, we modify them to accommodate the data transfer
methods investigated in this paper.

In order to focus on the performance of data transfers between the host
and the device memory, we allocate a data buffer to the pinned host
memory rather than the typical heap allocated by \texttt{malloc()}.
This pinned host memory space is mapped to the PCIe address and is never
swapped out.
It is also accessible to the GPU directly.

Our computing platform contains a single set of the CPU and the GPU.
Although we restrict our attention to CUDA and the GPU, the notion of
the investigated data transfer methods is well applicable to other
heterogeneous compute devices.
GPUs are currently well-recognized forms of the heterogeneous compute
devices, but emerging alternatives include the Intel Many Integrated
Core (MIC) and the AMD Fusion technology.
The programming models of these different platforms are almost identical
in that the CPU controls the compute devices.
Our future work includes an integrated investigation of these different
platforms.

\begin{figure}[!t]
 \centering
 \includegraphics[width=0.49\textwidth]{figure/Method/pci_gpu.pdf}
 \caption{Block diagram of the target system.}
 \label{fig:pci_gpu}
\end{figure}

Figure~\ref{fig:pci_gpu} shows a summarized block diagram of the target
system.
The host computer consists of the CPU and the host memory connected via
the system I/O bus.
They are connected to the PCIe bus to which the GPU board is also
connected.
This means that the GPU is visible to the CPU as a PCIe device.
The GPU is a complex compute device integrating a lot of hardware
functional units.
We focus on the CUDA-related units.
There are the device memory and the GPU chip connected through a high
bandwidth memory bus.
The GPU chip contains graphics processing clusters (GPCs), each of which
integrates hundreds of processing cores, \textit{a.k.a}, CUDA cores.
The number of GPCs and CUDA cores is architecture-specific.
For example, GPUs based on the NVIDIA GeForce Fermi
architecture~\cite{NVIDIA_Fermi} used in this paper suppor at most 4
GPCs and 512 CUDA cores.
Each GPC is configured by an on-chip microcontroller.
This microcontroller is wimpy but is capable of executing firmware code
with its own instruction set.
There is also a special \textit{hub} microcontroller, which broadcasts the
operations on all the GPC-dedicated microcontrollers.
The hardware DMA engines are also integrated on a chip.
We investigate how these hardware components operate and interact to
support data transfers in GPU computing.